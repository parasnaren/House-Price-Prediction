{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team ByteMe notebook for OBL Data Innovation Challenge\n",
    "\n",
    "   In this notebook we have described and implemented methods through which we were able to generate our regression models for the prediction task. We have used a combination of supervised ML models and Neural networks to compare the results. Stacked Average modelling was also done for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew\n",
    "from sklearn.linear_model import ElasticNet, Lasso, BayesianRidge, LassoLarsIC\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y, y_pred))\n",
    "\n",
    "def plot_vs_y(col):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(x = df[col], y = df['Price'])\n",
    "    plt.ylabel('Price', fontsize=13)\n",
    "    plt.xlabel(col, fontsize=13)\n",
    "    plt.show()\n",
    "\n",
    "def normal_dist(dataset, col, transform='None'):\n",
    "    df = dataset.copy()\n",
    "    if transform == 'log':\n",
    "        df[col] = np.log(df[col])\n",
    "    elif transform == 'boxcox':\n",
    "        df[col], fitted_lambda = stats.boxcox(df['Price'])\n",
    "        print('Fitted lambda for boxcox:', fitted_lambda)\n",
    "    \n",
    "    sns.distplot(df[col], fit=norm);\n",
    "    (mu, sigma) = norm.fit(df[col])\n",
    "    print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "    plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "                loc='best')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('SalePrice distribution')\n",
    "    fig = plt.figure()\n",
    "    res = stats.probplot(df[col], plot=plt)\n",
    "    plt.show()\n",
    "    \n",
    "def train_model(model, x_train, y_train, x_val):\n",
    "    model.fit(x_train, y_train)\n",
    "    return model.predict(x_val).astype(int), model\n",
    "\n",
    "def predict_test(model, filename, nn=False):\n",
    "    x_test = pd.read_csv('test_obl.csv').drop('id', axis=1)\n",
    "    x_test = extract_features(x_test)\n",
    "    x_test.columns = x_train.columns\n",
    "    if nn==True:\n",
    "        pred = model.predict(x_test.values).astype(int)\n",
    "    else:\n",
    "        pred = model.predict(x_test).astype(int)\n",
    "    create_submission(pred, filename)\n",
    "    \n",
    "def create_submission(pred, filename):\n",
    "    sample = pd.read_csv('sample.csv')\n",
    "    sample['Price'] = pred\n",
    "    sample.to_csv(filename, index=False)\n",
    "    print('Created file:', filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('training_obl.csv').drop('id', axis=1)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "ax = sns.heatmap(\n",
    "    corr, \n",
    "    vmin=-1, vmax=1, center=0,\n",
    "    cmap=sns.diverging_palette(20, 220, n=200),\n",
    "    square=True\n",
    ")\n",
    "ax.set_xticklabels(\n",
    "    ax.get_xticklabels(),\n",
    "    rotation=45,\n",
    "    horizontalalignment='right'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the heatmap it can be observed that the `Nbedrooms, NWashrooms, ANB, Grade` are the only features that have a positive correlation with the `Price` of the houses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_dist(df, 'Area(total)')\n",
    "normal_dist(df, 'Area(total)', 'log')\n",
    "normal_dist(df, 'Area(total)', 'boxcox')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Area(total)` does not follow a normal distribution and hence a form of transformation of the data will be required. The different areas are totalled and combined into a single column called `Area`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Area(total)'] = df['Area(total)'] + df['Roof(Area)'] + df['Lawn(Area)']\n",
    "normal_dist(df, 'Area(total)')\n",
    "normal_dist(df, 'Area(total)', 'log')\n",
    "normal_dist(df, 'Area(total)', 'boxcox')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that the `Area` column now follows a normal distribution and hence we can proceed with this transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_dist(df, 'Price')\n",
    "normal_dist(df, 'Price', 'log')\n",
    "normal_dist(df, 'Price', 'boxcox')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Due to the bimodal nature of the distribution of `Price` column, we worked with transformation using *log* and *boxcox*.**\n",
    "\n",
    "**However the results obtained did not better the ones obtained using non transformed features and was hence omitted from this notebook.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df['API'], bins=7)\n",
    "api_counts = pd.DataFrame(df['API'].value_counts())\n",
    "print('Unique values of API:', df['API'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`API` can be divided into 7 bins based on the distribution and the numerical attribute can be converted into a discrete categorical attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_counts.sort_index(inplace=False)\n",
    "pd.cut(df['API'], 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features selected are as shown below: \n",
    "1. `API` feature was dropped as it did not provide correlation with the `Price`.\n",
    "2. `Nbedrooms` was squared to provide for higher weightage\n",
    "3. `Rooms` feature was created by taking sum of the features `Nbedrooms` and `Nwashrooms`\n",
    "4. `Floors` was converted into categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1]\n",
    "y = df['Price']\n",
    "\n",
    "def extract_features(X):\n",
    "    X['Nbedrooms'] = X['Nbedrooms']**2\n",
    "    X['Rooms'] = X['Nbedrooms'] + X['Nwashrooms']\n",
    "    floors = pd.get_dummies(X['Nfloors'])\n",
    "    floors.columns = ['floors_'+str(i) for i in range(floors.shape[1])]\n",
    "    \n",
    "    #anb = pd.get_dummies(X['ANB'])\n",
    "    #anb.columns = ['anb_'+str(i) for i in range(anb.shape[1])]\n",
    "    \n",
    "    #api = pd.get_dummies(X['API'])\n",
    "    #api = pd.get_dummies(pd.cut(X['API'], 7, labels=['api_'+str(i) for i in range(7)]))\n",
    "    #X = pd.concat([X, floors, api], axis=1)\n",
    "    X = pd.concat([X, floors], axis=1)\n",
    "    X.drop(['Roof(Area)','Lawn(Area)','Nfloors','API'], axis=1, inplace=True)\n",
    "    #X.drop(['Roof(Area)','Lawn(Area)','Nfloors','API','Grade','ANB'], axis=1, inplace=True)\n",
    "    return X\n",
    "\n",
    "X = extract_features(X.copy())\n",
    "#y_log = np.log(y)\n",
    "x_train, x_val, y_train, y_val = tts(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Models\n",
    "\n",
    "1. Lasso Regression\n",
    "2. ElasticNet\n",
    "3. Kernel Ridge\n",
    "4. Decision Tree\n",
    "5. Random Forest\n",
    "6. Gradient Boosting\n",
    "7. XGBoost\n",
    "8. Light Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "\n",
    "def rmsle_cv(model):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(x_train.values)\n",
    "    rmse = np.sqrt(-cross_val_score(model, x_train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lasso = make_pipeline(RobustScaler(), Lasso(alpha=0.0005, random_state=1))\n",
    "score = rmsle_cv(lasso)\n",
    "print(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n",
    "score = rmsle_cv(ENet)\n",
    "print(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n",
    "score = rmsle_cv(KRR)\n",
    "print(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeRegressor(random_state=0)\n",
    "score = rmsle_cv(tree)\n",
    "print(\"Decision Tree score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "score = rmsle_cv(forest)\n",
    "print(\"Random Forest(100) score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =5)\n",
    "score = rmsle_cv(GBoost)\n",
    "print(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
    "                             learning_rate=0.05, max_depth=3, \n",
    "                             min_child_weight=1.7817, n_estimators=2200,\n",
    "                             reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "                             subsample=0.5213, silent=1,\n",
    "                             random_state =7, nthread = -1)\n",
    "score = rmsle_cv(model_xgb)\n",
    "print(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n",
    "                              learning_rate=0.05, n_estimators=720,\n",
    "                              max_bin = 55, bagging_fraction = 0.8,\n",
    "                              bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                              feature_fraction_seed=9, bagging_seed=9,\n",
    "                              min_data_in_leaf=6, min_sum_hessian_in_leaf = 11)\n",
    "score = rmsle_cv(model_lgb)\n",
    "print(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models used for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBoost = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
    "                             learning_rate=0.05, max_depth=3, \n",
    "                             min_child_weight=1.7817, n_estimators=10000,\n",
    "                             reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "                             subsample=0.5213, silent=1,\n",
    "                             random_state=7, nthread = -1)\n",
    "xgb_pred, xgb_model = train_model(XGBoost, x_train, y_train, x_val)\n",
    "print('XGBoost RMSE:', rmse(xgb_pred, y_val))\n",
    "predict_test(xgb_model, 'xgb-pred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LGBoost = lgb.LGBMRegressor(objective='regression', \n",
    "                            num_leaves=5,\n",
    "                            learning_rate=0.05,\n",
    "                            n_estimators=50000,\n",
    "                            max_bin=55,\n",
    "                            bagging_fraction=0.8,\n",
    "                            bagging_freq=5,\n",
    "                            feature_fraction=0.2319,\n",
    "                            feature_fraction_seed=9,\n",
    "                            bagging_seed=9,\n",
    "                            min_data_in_leaf=6,\n",
    "                            min_sum_hessian_in_leaf=11)\n",
    "lgb_pred, lgb_model = train_model(LGBoost, x_train, y_train, x_val)\n",
    "print('LGBoost RMSE:', rmse(lgb_pred, y_val))\n",
    "predict_test(xgb_model, 'lgb-pred.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_model = Sequential()\n",
    "\n",
    "NN_model.add(Dense(128, kernel_initializer='normal',input_dim = x_train.shape[1], activation='relu'))\n",
    "\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "\n",
    "NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
    "\n",
    "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "NN_model.summary()\n",
    "\n",
    "\n",
    "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_model.fit(x_train.values, y_train, epochs=500, batch_size=32, validation_split=0.2, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = NN_model.predict(x_val.values)\n",
    "print('NN RMSE: ', rmse(y_pred, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_file = 'Weights-480--259.85519.hdf5'\n",
    "NN_model.load_weights(weights_file)\n",
    "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = NN_model.predict(x_val.values)\n",
    "print('Best Neural Network RMSE: ', rmse(y_pred, y_val))\n",
    "predict_test(NN_model, 'neural-net-pred.csv', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_val, y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked Regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, base_models, meta_model, n_folds=5):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "   \n",
    "    def fit(self, X, y):\n",
    "        self.base_models_ = [list() for x in self.base_models]\n",
    "        self.meta_model_ = clone(self.meta_model)\n",
    "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n",
    "        \n",
    "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            for train_index, holdout_index in kfold.split(X, y):\n",
    "                instance = clone(model)\n",
    "                self.base_models_[i].append(instance)\n",
    "                instance.fit(X[train_index], y[train_index])\n",
    "                y_pred = instance.predict(X[holdout_index])\n",
    "                out_of_fold_predictions[holdout_index, i] = y_pred\n",
    "                        \n",
    "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "        return self\n",
    "   \n",
    "    def predict(self, X):\n",
    "        meta_features = np.column_stack([\n",
    "            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n",
    "            for base_models in self.base_models_ ])\n",
    "        return self.meta_model_.predict(meta_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stacked_averaged_models = StackingAveragedModels(base_models = (GBoost, LGBoost),\n",
    "                                                 meta_model = XGBoost)\n",
    "stacked_pred, stacked_model = train_model(stacked_averaged_models, x_train.values, y_train.values, x_val.values)\n",
    "print('Stacked model RMSE:', rmse(stacked_pred, y_val))\n",
    "predict_test(stacked_model, 'stacked-pred.csv', True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
